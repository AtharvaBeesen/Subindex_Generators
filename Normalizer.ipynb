{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1393403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "def calculate_percentiles(file_name, column_name):\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_name)\n",
    "    \n",
    "    # Ensure column is numeric\n",
    "    data[column_name] = pd.to_numeric(data[column_name], errors='coerce')\n",
    "    \n",
    "    # Create a new column for the percentile\n",
    "    subindex = file_name.split(\"_\")[0]  # Get subindex (e.g., CDI, IDI)\n",
    "    \n",
    "    # Calculate percentile ranks only for non-null values\n",
    "    data[subindex] = data[column_name].rank(pct=True) # Percentile scores (0 to 100)\n",
    "    \n",
    "    # Ensure that NaN values in the original column result in NaN in the percentile column\n",
    "    data.loc[data[column_name].isnull(), subindex] = None\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_file(file_name, column_name):\n",
    "    # Calculate percentiles\n",
    "    processed_data = calculate_percentiles(file_name, column_name)\n",
    "    \n",
    "    # Split GEOID into GEOID and Year\n",
    "    processed_data[['GEOID', 'Year']] = processed_data['GEOID'].str.split('+', expand=True)\n",
    "    processed_data['Year'] = processed_data['Year'].astype(int)\n",
    "    \n",
    "    # Iterate over unique years and save the data\n",
    "    for year in processed_data['Year'].unique():\n",
    "        year_data = processed_data[processed_data['Year'] == year]\n",
    "        subindex = file_name.split(\"_\")[0]  # Get CDI/IDI/LDI/PDI from filename\n",
    "        \n",
    "        # Prepare filenames\n",
    "        csv_filename = f'block_groups_{year}_{subindex}.csv'\n",
    "        geojson_filename = f'block_groups_{year}_{subindex}.geojson'\n",
    "        \n",
    "        # Update CSV\n",
    "        if os.path.exists(csv_filename):\n",
    "            csv_data = pd.read_csv(csv_filename)\n",
    "            \n",
    "            year_data['GEOID'] = year_data['GEOID'].astype('int64')\n",
    "            csv_data[subindex] = csv_data['GEOID'].map(year_data.set_index('GEOID')[subindex])\n",
    "            \n",
    "            # Save updated CSV back to disk\n",
    "            csv_data.to_csv(csv_filename, index=False)\n",
    "        \n",
    "        # Update GeoJSON\n",
    "        if os.path.exists(geojson_filename):\n",
    "            geojson_data = gpd.read_file(geojson_filename)\n",
    "            geojson_data[subindex] = geojson_data['GEOID'].map(year_data.set_index('GEOID')[subindex])\n",
    "            geojson_data.to_file(geojson_filename, driver=\"GeoJSON\")\n",
    "\n",
    "def main():\n",
    "    files_to_process = [\n",
    "        (\"CDI_bg_all.csv\", \"Commercial Density\"),\n",
    "        (\"IDI_bg_all.csv\", \"Intersection Density\"),\n",
    "        (\"LDI_bg_all.csv\", \"Entropy\"),\n",
    "        (\"PDI_bg_all.csv\", \"Population Density\")\n",
    "    ]\n",
    "    \n",
    "    for file_name, column_name in files_to_process:\n",
    "        if os.path.exists(file_name):\n",
    "            process_file(file_name, column_name)\n",
    "        else:\n",
    "            print(f\"File not found: {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
